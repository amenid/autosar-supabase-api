import asyncio
import streamlit as st

# CONFIGURATION INITIALE - DOIT ÃŠTRE EN PREMIER
st.set_page_config(
    page_title="ğŸš— AUTOSAR RAG Assistant",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Configuration AsyncIO
try:
    asyncio.set_event_loop(asyncio.new_event_loop())
except:
    pass

# Imports principaux
import os
import torch
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders
import ssl
import time
import re
from typing import List, Dict, Any, Optional
import logging
import secrets

# Configuration Torch
try:
    torch._C._disable_internal_usage_checks = True
except:
    pass

# Configuration Streamlit
try:
    st.set_option('server.fileWatcherType', 'none')
except:
    pass

# Imports pour RAG
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from pathlib import Path
import PyPDF2
import docx
import requests
import warnings

# Suppression des warnings
warnings.filterwarnings('ignore')

# Configuration globale EMAIL (du code 2)
SMTP_CONFIG = {
    'SMTP_SERVER': os.getenv('SMTP_SERVER', 'smtp-relay.brevo.com'),
    'SMTP_PORT': int(os.getenv('SMTP_PORT', 587)),
    'SMTP_USERNAME': os.getenv('SMTP_USERNAME', '7d7544008@smtp-brevo.com'),
    'SMTP_PASSWORD': os.getenv('SMTP_PASSWORD', 'JMjV80bfWNQhrPCK'),
    'FROM_EMAIL': os.getenv('FROM_EMAIL', 'ameniaydiii@gmail.com')
}

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chatbot.log'),
        logging.StreamHandler()
    ]
)

class DocumentChunk:
    """ReprÃ©sente un chunk de document avec mÃ©tadonnÃ©es"""
    def __init__(self, content: str, metadata: dict):
        self.content = content
        self.metadata = metadata
        self.properties = metadata  # Pour compatibilitÃ©

class RAGRetriever:
    """SystÃ¨me RAG complet pour AUTOSAR"""
    
    def __init__(self, documents_path: str = "autosar_documents"):
        self.documents_path = Path(documents_path)
        self.documents_path.mkdir(exist_ok=True)
        
        # ModÃ¨le d'embedding
        print("ğŸ”„ Chargement du modÃ¨le d'embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Base de donnÃ©es vectorielle
        self.vector_db_path = "autosar_vectors.faiss"
        self.metadata_path = "autosar_metadata.pkl"
        
        # Chunks et mÃ©tadonnÃ©es
        self.chunks = []
        self.chunk_embeddings = None
        self.faiss_index = None
        
        # Initialiser ou charger la base existante
        self._initialize_or_load_database()
        
        print(f"âœ… RAG initialisÃ© avec {len(self.chunks)} chunks")
    
    def _initialize_or_load_database(self):
        """Initialise ou charge la base de donnÃ©es existante"""
        if os.path.exists(self.vector_db_path) and os.path.exists(self.metadata_path):
            print("ğŸ“‚ Chargement de la base vectorielle existante...")
            self._load_existing_database()
        else:
            print("ğŸ†• CrÃ©ation d'une nouvelle base vectorielle...")
            self._create_sample_autosar_content()
            self._build_vector_database()
    
    def _load_existing_database(self):
        """Charge la base vectorielle existante"""
        try:
            # Charger l'index FAISS
            self.faiss_index = faiss.read_index(self.vector_db_path)
            
            # Charger les mÃ©tadonnÃ©es
            with open(self.metadata_path, 'rb') as f:
                self.chunks = pickle.load(f)
            
            print(f"âœ… Base vectorielle chargÃ©e: {len(self.chunks)} chunks")
            
        except Exception as e:
            print(f"âŒ Erreur chargement base: {e}")
            self._create_sample_autosar_content()
            self._build_vector_database()
    
    def _create_sample_autosar_content(self):
        """CrÃ©e du contenu AUTOSAR de dÃ©monstration"""
        sample_content = {
            "AUTOSAR_Architecture_Overview.txt": """
AUTOSAR (AUTomotive Open System ARchitecture) Overview

AUTOSAR is a global partnership of automotive manufacturers, suppliers, and other companies from the electronics, semiconductor and software industry. The architecture consists of three main layers:

1. Application Layer
- Software Components (SWCs)
- AUTOSAR Runtime Environment (RTE)
- Complex Device Drivers (CDD)

2. Runtime Environment (RTE)
- Communication abstraction
- Service-oriented communication
- Event-driven communication

3. Basic Software (BSW)
- Operating System (OS)
- Communication Stack
- Memory Stack
- I/O Hardware Abstraction

Key Benefits:
- Standardization across automotive industry
- Improved software reusability
- Enhanced modularity and scalability
- Better integration of software components
""",
            
            "AUTOSAR_Communication_Stack.txt": """
AUTOSAR Communication Stack

The AUTOSAR communication stack enables efficient data exchange between ECUs:

1. CAN Communication
- Controller Area Network protocol
- Frame formats: Standard (11-bit) and Extended (29-bit)
- Error handling and arbitration mechanisms

2. Ethernet Communication
- IEEE 802.3 standard support
- TCP/IP and UDP protocols
- Time-sensitive networking (TSN)

3. LIN Communication
- Local Interconnect Network
- Master-slave architecture
- Low-cost communication for non-critical systems

Communication Services:
- Diagnostic Communication Manager (DCM)
- Network Management (NM)
- Communication Security (SecOC)
- Service Discovery (SD)

Protocol Data Units (PDUs):
- I-PDU: Interaction PDU
- N-PDU: Network PDU
- Transport Protocol handling
""",
            
            "RFC_Standards_Integration.txt": """
RFC Standards in AUTOSAR Context

AUTOSAR integrates various RFC standards for communication protocols:

RFC 791 - Internet Protocol (IP)
- IPv4 addressing and routing
- Packet fragmentation and reassembly
- Integration with AUTOSAR Ethernet stack

RFC 793 - Transmission Control Protocol (TCP)
- Reliable, connection-oriented communication
- Flow control and congestion management
- Used in AUTOSAR for diagnostic services

RFC 768 - User Datagram Protocol (UDP)
- Connectionless communication protocol
- Low overhead for real-time applications
- SOME/IP communication over UDP

Security RFCs:
- RFC 5246 (TLS 1.2)
- RFC 8446 (TLS 1.3)
- RFC 3280 (PKI Certificate validation)
""",
            
            "AUTOSAR_Security.txt": """
AUTOSAR Security Framework

Cybersecurity is crucial in modern automotive systems:

1. Secure Communication
- Message Authentication Codes (MAC)
- Encryption and decryption services
- Key management infrastructure

2. Secure Boot Process
- Verified boot chain
- Code integrity validation
- Hardware Security Module (HSM) integration

3. Intrusion Detection
- Anomaly detection algorithms
- Network traffic monitoring
- Incident response procedures

Security Modules:
- Cryptographic Service Manager (CSM)
- Secure Onboard Communication (SecOC)
- Key Manager (KeyM)
- Certificate Manager (CertM)

Compliance Standards:
- ISO/SAE 21434 (Cybersecurity Engineering)
- UN-R155 (Cybersecurity Management System)
- UN-R156 (Software Update Management System)
"""
        }
        
        # CrÃ©er les fichiers de dÃ©monstration
        for filename, content in sample_content.items():
            file_path = self.documents_path / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        
        print(f"ğŸ“ {len(sample_content)} documents AUTOSAR crÃ©Ã©s")
    
    def load_documents(self):
        """Charge tous les documents du rÃ©pertoire"""
        documents = []
        
        for file_path in self.documents_path.glob("*"):
            if file_path.suffix.lower() in ['.txt', '.pdf', '.docx', '.md']:
                try:
                    content = self._extract_text(file_path)
                    if content.strip():
                        documents.append({
                            'content': content,
                            'source': file_path.name,
                            'path': str(file_path)
                        })
                        print(f"âœ… ChargÃ©: {file_path.name}")
                except Exception as e:
                    print(f"âŒ Erreur chargement {file_path.name}: {e}")
        
        print(f"ğŸ“š {len(documents)} documents chargÃ©s")
        return documents
    
    def _extract_text(self, file_path: Path) -> str:
        """Extrait le texte selon le type de fichier"""
        if file_path.suffix.lower() == '.txt' or file_path.suffix.lower() == '.md':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        elif file_path.suffix.lower() == '.pdf':
            text = ""
            try:
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        text += page.extract_text() + "\n"
            except:
                print(f"âš ï¸ Erreur lecture PDF: {file_path.name}")
            return text
        
        elif file_path.suffix.lower() == '.docx':
            try:
                doc = docx.Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text
            except:
                print(f"âš ï¸ Erreur lecture DOCX: {file_path.name}")
                return ""
        
        return ""
    
    def create_chunks(self, documents: List[Dict]) -> List[DocumentChunk]:
        """Divise les documents en chunks avec overlap"""
        chunks = []
        chunk_size = 500
        overlap = 50
        
        for doc in documents:
            content = doc['content']
            words = content.split()
            
            for i in range(0, len(words), chunk_size - overlap):
                chunk_words = words[i:i + chunk_size]
                chunk_content = ' '.join(chunk_words)
                
                if len(chunk_content.strip()) > 50:
                    chunk = DocumentChunk(
                        content=chunk_content,
                        metadata={
                            'source': doc['source'],
                            'path': doc['path'],
                            'chunk_index': len(chunks),
                            'word_count': len(chunk_words)
                        }
                    )
                    chunks.append(chunk)
        
        print(f"âœ‚ï¸ {len(chunks)} chunks crÃ©Ã©s")
        return chunks
    
    def _build_vector_database(self):
        """Construit la base de donnÃ©es vectorielle"""
        # Charger les documents
        documents = self.load_documents()
        if not documents:
            print("âš ï¸ Aucun document trouvÃ©")
            return
        
        # CrÃ©er les chunks
        self.chunks = self.create_chunks(documents)
        if not self.chunks:
            print("âš ï¸ Aucun chunk crÃ©Ã©")
            return
        
        # GÃ©nÃ©rer les embeddings
        print("ğŸ”„ GÃ©nÃ©ration des embeddings...")
        chunk_texts = [chunk.content for chunk in self.chunks]
        embeddings = self.embedding_model.encode(chunk_texts, show_progress_bar=True)
        
        # CrÃ©er l'index FAISS
        dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)
        
        # Normaliser pour cosine similarity
        faiss.normalize_L2(embeddings)
        self.faiss_index.add(embeddings.astype('float32'))
        
        # Sauvegarder
        faiss.write_index(self.faiss_index, self.vector_db_path)
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(self.chunks, f)
        
        print(f"ğŸ’¾ Base vectorielle sauvegardÃ©e avec {len(self.chunks)} chunks")
    
    def hybrid_search(self, query: str, top_k: int = 5) -> List[DocumentChunk]:
        """Recherche hybride (vectorielle + mot-clÃ©s)"""
        if not self.faiss_index or not self.chunks:
            print("âš ï¸ Base vectorielle non initialisÃ©e")
            return []
        
        # Recherche vectorielle
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), top_k * 2)
        
        # RÃ©cupÃ©rer les chunks
        vector_results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                vector_results.append((chunk, float(score)))
        
        # Recherche par mots-clÃ©s
        query_words = set(query.lower().split())
        keyword_results = []
        
        for chunk in self.chunks:
            chunk_words = set(chunk.content.lower().split())
            overlap = len(query_words.intersection(chunk_words))
            if overlap > 0:
                keyword_score = overlap / len(query_words)
                keyword_results.append((chunk, keyword_score))
        
        # Combiner les rÃ©sultats
        combined_scores = {}
        
        # Ajouter scores vectoriels
        for chunk, score in vector_results:
            chunk_id = id(chunk)
            combined_scores[chunk_id] = {'chunk': chunk, 'vector_score': score, 'keyword_score': 0}
        
        # Ajouter scores mots-clÃ©s
        for chunk, score in keyword_results:
            chunk_id = id(chunk)
            if chunk_id in combined_scores:
                combined_scores[chunk_id]['keyword_score'] = score
            else:
                combined_scores[chunk_id] = {'chunk': chunk, 'vector_score': 0, 'keyword_score': score}
        
        # Score final combinÃ©
        final_results = []
        for data in combined_scores.values():
            final_score = 0.7 * data['vector_score'] + 0.3 * data['keyword_score']
            final_results.append((data['chunk'], final_score))
        
        # Trier par score final
        final_results.sort(key=lambda x: x[1], reverse=True)
        
        return [chunk for chunk, score in final_results[:top_k]]
    
    def generate_answer(self, query: str, chunks: List[DocumentChunk], 
                       model_type: str = "deepseek-r1:7b", api_key: str = None, 
                       temperature: float = 0.3) -> str:
        """GÃ©nÃ¨re une rÃ©ponse basÃ©e sur les chunks rÃ©cupÃ©rÃ©s"""
        
        if not chunks:
            return "âŒ Aucun contexte pertinent trouvÃ© dans la base de connaissances AUTOSAR."
        
        # Construire le contexte
        context = "\n\n".join([
            f"ğŸ“„ **Source: {chunk.metadata['source']}**\n{chunk.content}"
            for chunk in chunks
        ])
        
        # Prompt systÃ¨me pour AUTOSAR
        system_prompt = """Tu es un expert AUTOSAR (AUTomotive Open System ARchitecture) et des standards RFC. 
        RÃ©ponds de maniÃ¨re prÃ©cise et technique en utilisant UNIQUEMENT les informations fournies dans le contexte.
        
        Instructions:
        - Utilise le contexte fourni pour rÃ©pondre
        - Cite les sources quand nÃ©cessaire
        - Sois prÃ©cis et technique
        - Si l'information n'est pas dans le contexte, dis-le clairement
        - Formate ta rÃ©ponse en markdown pour une meilleure lisibilitÃ©"""
        
        # Prompt utilisateur
        user_prompt = f"""
        **Question:** {query}
        
        **Contexte AUTOSAR disponible:**
        {context}
        
        **RÃ©ponse dÃ©taillÃ©e:**
        """
        
        try:
            if "API Model" in model_type and api_key:
                return self._call_api_model(system_prompt, user_prompt, api_key, temperature)
            else:
                return self._call_ollama_model(system_prompt, user_prompt, model_type, temperature)
        
        except Exception as e:
            print(f"âŒ Erreur gÃ©nÃ©ration rÃ©ponse: {e}")
            return f"âŒ Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse: {str(e)}"
    
    def _call_ollama_model(self, system_prompt: str, user_prompt: str, 
                          model: str, temperature: float) -> str:
        """Appelle un modÃ¨le Ollama local"""
        try:
            # URL Ollama par dÃ©faut
            ollama_url = "http://localhost:11434/api/generate"
            
            payload = {
                "model": model,
                "prompt": f"{system_prompt}\n\n{user_prompt}",
                "temperature": temperature,
                "stream": False
            }
            
            response = requests.post(ollama_url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'RÃ©ponse vide du modÃ¨le')
            else:
                return f"âŒ Erreur Ollama ({response.status_code}): Assurez-vous qu'Ollama est dÃ©marrÃ© avec `ollama serve`"
        
        except requests.exceptions.ConnectionError:
            return """âŒ **Impossible de se connecter Ã  Ollama**
            
            **Solutions:**
            1. Installez Ollama: https://ollama.com/
            2. DÃ©marrez le service: `ollama serve`
            3. TÃ©lÃ©chargez le modÃ¨le: `ollama pull deepseek-r1:7b`
            4. Ou utilisez un modÃ¨le API avec votre clÃ©"""
        
        except Exception as e:
            return f"âŒ Erreur inattendue: {str(e)}"
    
    def _call_api_model(self, system_prompt: str, user_prompt: str, 
                       api_key: str, temperature: float) -> str:
        """Appelle un modÃ¨le via API"""
        try:
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            return f"RÃ©ponse basÃ©e sur l'API : {full_prompt[:200]}..."
        
        except Exception as e:
            return f"âŒ Erreur API: {str(e)}"

class EmailManager:
    """Gestionnaire d'emails avec SMTP Relay (du code 2)"""
    
    def __init__(self):
        self.smtp_server = SMTP_CONFIG['SMTP_SERVER']
        self.smtp_port = SMTP_CONFIG['SMTP_PORT']
        self.email = SMTP_CONFIG['FROM_EMAIL']
        self.username = SMTP_CONFIG['SMTP_USERNAME']
        self.password = SMTP_CONFIG['SMTP_PASSWORD']
    
    def send_email(self, to_email: str, subject: str, body: str, 
                   is_html: bool = False, attachments: List[str] = None) -> tuple[bool, str]:
        """Envoie un email avec piÃ¨ces jointes optionnelles"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.email
            msg['To'] = to_email
            msg['Subject'] = subject
            
            # Corps du message
            if is_html:
                msg.attach(MIMEText(body, 'html'))
            else:
                msg.attach(MIMEText(body, 'plain'))
            
            # PiÃ¨ces jointes
            if attachments:
                for file_path in attachments:
                    if os.path.exists(file_path):
                        with open(file_path, "rb") as attachment:
                            part = MIMEBase('application', 'octet-stream')
                            part.set_payload(attachment.read())
                        
                        encoders.encode_base64(part)
                        part.add_header(
                            'Content-Disposition',
                            f'attachment; filename= {os.path.basename(file_path)}'
                        )
                        msg.attach(part)
            
            # Envoi via SMTP Relay
            context = ssl.create_default_context()
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls(context=context)
                server.login(self.username, self.password)
                server.send_message(msg)
            
            logging.info(f"Email envoyÃ© avec succÃ¨s Ã  {to_email}")
            return True, "Email envoyÃ© avec succÃ¨s"
        
        except Exception as e:
            logging.error(f"Erreur lors de l'envoi d'email: {e}")
            return False, f"Erreur: {str(e)}"
    
    def send_conversation_email(self, to_email: str, messages: List[Dict]) -> tuple[bool, str]:
        """Envoie la conversation par email (du code 2)"""
        subject = f"ğŸš— Conversation AUTOSAR RAG - {datetime.now().strftime('%d/%m/%Y %H:%M')}"
        
        # CrÃ©er le contenu HTML
        html_content = self._generate_conversation_html(messages)
        
        # CrÃ©er aussi une version texte
        text_content = self._generate_conversation_text(messages)
        
        return self.send_email(to_email, subject, html_content, is_html=True)
    
    def send_source_code_email(self, to_email: str) -> tuple[bool, str]:
        """Envoie le code source par email"""
        subject = "ğŸš— Code Source AUTOSAR RAG Assistant"
        
        try:
            # Lire le code source actuel
            with open(__file__, 'r', encoding='utf-8') as f:
                source_code = f.read()
            
            body = f"""
Bonjour,

Voici le code source complet de l'application AUTOSAR RAG Assistant.

Date de gÃ©nÃ©ration: {datetime.now().strftime('%d/%m/%Y Ã  %H:%M')}

FonctionnalitÃ©s incluses:
- ğŸ¤– SystÃ¨me RAG avec FAISS et sentence-transformers
- ğŸ“„ Support multi-formats (PDF, DOCX, TXT, MD)
- ğŸ” Recherche hybride (vectorielle + mots-clÃ©s)
- ğŸ“§ Envoi d'emails via SMTP Relay
- ğŸš— Base de connaissances AUTOSAR prÃ©-construite

Code source:
{'='*50}

{source_code}

{'='*50}

Cordialement,
L'Ã©quipe AUTOSAR RAG Assistant
            """
            
            return self.send_email(to_email, subject, body)
            
        except Exception as e:
            return False, f"Erreur lors de la lecture du code source: {str(e)}"
    
    def _generate_conversation_html(self, messages: List[Dict]) -> str:
        """GÃ©nÃ¨re le HTML de la conversation"""
        current_time = datetime.now().strftime("%d/%m/%Y Ã  %H:%M")
        
        html = f"""
        <html>
        <head>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .message {{ margin: 15px 0; padding: 10px; border-radius: 5px; }}
                .user {{ background-color: #e3f2fd; }}
                .assistant {{ background-color: #f1f8e9; }}
                .timestamp {{ font-size: 0.8em; color: #666; }}
                .header {{ background-color: #1976d2; color: white; padding: 15px; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h2>ğŸš— Conversation AUTOSAR RAG Assistant</h2>
                <p><strong>GÃ©nÃ©rÃ© le:</strong> {current_time}</p>
            </div>
        """
        
        for message in messages:
            role_name = "ğŸ‘¤ Vous" if message["role"] == "user" else "ğŸ¤– Assistant"
            css_class = message["role"]
            
            # Remplacer les retours Ã  la ligne par des <br>
            content_with_br = message["content"].replace('\n', '<br>')
            
            html += f"""
            <div class="message {css_class}">
                <strong>{role_name}:</strong><br>
                {content_with_br}
                <div class="timestamp">{message.get('timestamp', datetime.now())}</div>
            </div>
            """
        
        html += """
            <hr>
            <p><em>Email gÃ©nÃ©rÃ© automatiquement par AUTOSAR RAG Assistant</em></p>
            </body>
        </html>
        """
        
        return html
    
    def _generate_conversation_text(self, messages: List[Dict]) -> str:
        """GÃ©nÃ¨re la version texte de la conversation"""
        text = f"ğŸš— Conversation AUTOSAR RAG Assistant\n"
        text += f"GÃ©nÃ©rÃ© le: {datetime.now().strftime('%d/%m/%Y Ã  %H:%M')}\n"
        text += "="*50 + "\n\n"
        
        for message in messages:
            role = "Vous" if message["role"] == "user" else "Assistant"
            text += f"{role}: {message['content']}\n\n"
        
        text += "="*50 + "\n"
        text += "Email gÃ©nÃ©rÃ© automatiquement par AUTOSAR RAG Assistant"
        
        return text

def init_session_state():
    """Initialise les variables de session"""
    # RAG System
    if 'rag' not in st.session_state:
        with st.spinner("ğŸ”„ Initialisation du systÃ¨me RAG AUTOSAR..."):
            st.session_state.rag = RAGRetriever()
    
    # Email Manager
    if 'email_manager' not in st.session_state:
        st.session_state.email_manager = EmailManager()
    
    # Messages et conversations
    if 'messages' not in st.session_state:
        st.session_state.messages = []

def main():
    """Application principale simplifiÃ©e avec RAG et emails"""
    
    # CSS personnalisÃ©
    st.markdown("""
    <style>
        .main { padding-top: 0rem; }
        .email-box { 
            background-color: #f0f2f6; 
            padding: 15px; 
            border-radius: 10px; 
            border-left: 4px solid #1976d2; 
            margin: 10px 0;
        }
        .success-box { 
            background-color: #e8f5e8; 
            padding: 15px; 
            border-radius: 10px; 
            border-left: 4px solid #4caf50; 
            margin: 10px 0;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # Initialisation
    init_session_state()
    
    # Header principal
    st.title("ğŸš— AUTOSAR RAG Assistant")
    st.markdown("### ğŸ¤– Assistant intelligent pour AUTOSAR et standards RFC")
    
    # Sidebar avec paramÃ¨tres
    with st.sidebar:
        st.header("âš™ï¸ Configuration")
        
        selected_model = st.selectbox(
            "ğŸ¤– Choisir le modÃ¨le",
            ["deepseek-r1:7b", "llama3.1:latest", "API Model (Gemini)"],
            index=0,
            help="SÃ©lectionnez le modÃ¨le pour gÃ©nÃ©rer les rÃ©ponses"
        )
        
        api_key = None
        if "API Model" in selected_model:
            api_key = st.text_input("ğŸ”‘ ClÃ© API", type="password")
        
        temperature = st.slider("ğŸŒ¡ï¸ TempÃ©rature", 0.0, 1.0, 0.3, help="ContrÃ´le la crÃ©ativitÃ© des rÃ©ponses")
        
        st.divider()
        
        # Statistiques RAG
        st.header("ğŸ“Š Statistiques RAG")
        if hasattr(st.session_state, 'rag') and st.session_state.rag.chunks:
            stats = {
                'total_chunks': len(st.session_state.rag.chunks),
                'total_documents': len(set(chunk.metadata['source'] for chunk in st.session_state.rag.chunks)),
                'embedding_model': 'all-MiniLM-L6-v2'
            }
            st.metric("ğŸ“š Documents", stats['total_documents'])
            st.metric("ğŸ§© Chunks", stats['total_chunks'])
            st.caption(f"ğŸ§  ModÃ¨le: {stats['embedding_model']}")
        
        st.divider()
        
        # Section Email (du code 2)
        st.header("ğŸ“§ Fonctions Email")
        
        # Zone email mise en Ã©vidence
        st.markdown('<div class="email-box">', unsafe_allow_html=True)
        st.subheader("âœ‰ï¸ Envoyer par Email")
        
        email_recipient = st.text_input(
            "ğŸ“® Adresse email destinataire",
            placeholder="votre.email@example.com",
            help="Tapez l'email et appuyez sur les boutons ci-dessous"
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“§ Envoyer Conversation", type="primary"):
                if email_recipient and st.session_state.messages:
                    with st.spinner("ğŸ“¤ Envoi de la conversation..."):
                        success, message = st.session_state.email_manager.send_conversation_email(
                            email_recipient, st.session_state.messages
                        )
                        
                        if success:
                            st.markdown('<div class="success-box">âœ… <strong>Conversation envoyÃ©e avec succÃ¨s!</strong></div>', unsafe_allow_html=True)
                        else:
                            st.error(f"âŒ Erreur: {message}")
                elif not email_recipient:
                    st.error("âŒ Veuillez saisir une adresse email")
                else:
                    st.error("âŒ Aucune conversation Ã  envoyer")
        
        with col2:
            if st.button("ğŸ’» Envoyer Code Source", type="secondary"):
                if email_recipient:
                    with st.spinner("ğŸ“¤ Envoi du code source..."):
                        success, message = st.session_state.email_manager.send_source_code_email(email_recipient)
                        
                        if success:
                            st.markdown('<div class="success-box">âœ… <strong>Code source envoyÃ©!</strong></div>', unsafe_allow_html=True)
                        else:
                            st.error(f"âŒ Erreur: {message}")
                else:
                    st.error("âŒ Veuillez saisir une adresse email")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        st.divider()
        
        # Upload de documents
        st.header("ğŸ“ Ajouter Documents")
        uploaded_file = st.file_uploader(
            "ğŸ“¤ Ajouter un document AUTOSAR",
            type=['txt', 'pdf', 'docx', 'md'],
            help="Ajoutez vos propres documents AUTOSAR"
        )
        
        if uploaded_file:
            if st.button("âœ… Ajouter au RAG"):
                with st.spinner("Traitement du document..."):
                    try:
                        # Sauvegarder le fichier
                        file_path = st.session_state.rag.documents_path / uploaded_file.name
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        
                        # Reconstruire la base
                        st.session_state.rag._build_vector_database()
                        st.success(f"âœ… Document {uploaded_file.name} ajoutÃ©!")
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"âŒ Erreur: {e}")
    
    # Interface de chat principale
    st.subheader("ğŸ’¬ Chat AUTOSAR avec RAG")
    
    # Affichage des messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                st.caption("ğŸ” RÃ©ponse basÃ©e sur la base de connaissances AUTOSAR")
    
    # Input utilisateur
    if prompt := st.chat_input("ğŸ’­ Posez votre question sur AUTOSAR ou les RFC..."):
        
        # Ajouter le message utilisateur avec timestamp
        user_message = {
            "role": "user", 
            "content": prompt,
            "timestamp": datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        }
        st.session_state.messages.append(user_message)
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # GÃ©nÃ©rer la rÃ©ponse avec RAG
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Recherche dans la base AUTOSAR..."):
                
                # Recherche RAG
                start_time = time.time()
                chunks = st.session_state.rag.hybrid_search(prompt, top_k=5)
                search_time = time.time() - start_time
                
                st.caption(f"ğŸ” {len(chunks)} chunks trouvÃ©s en {search_time:.2f}s")
                
                # GÃ©nÃ©ration de la rÃ©ponse
                with st.spinner("ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse..."):
                    answer = st.session_state.rag.generate_answer(
                        prompt,
                        chunks,
                        model_type=selected_model,
                        api_key=api_key,
                        temperature=temperature
                    )
                
                # Affichage de la rÃ©ponse
                st.markdown(answer)
                st.caption("ğŸ” RÃ©ponse basÃ©e sur la base de connaissances AUTOSAR")
                
                # Sources utilisÃ©es
                if chunks:
                    with st.expander("ğŸ“š Sources consultÃ©es"):
                        for i, chunk in enumerate(chunks, 1):
                            st.write(f"**{i}. {chunk.metadata['source']}** ({chunk.metadata['word_count']} mots)")
                            with st.expander(f"Extrait {i}"):
                                st.text(chunk.content[:300] + "..." if len(chunk.content) > 300 else chunk.content)
                
                # Ajouter Ã  l'historique avec timestamp
                assistant_message = {
                    "role": "assistant", 
                    "content": answer,
                    "timestamp": datetime.now().strftime("%d/%m/%Y %H:%M:%S")
                }
                st.session_state.messages.append(assistant_message)
    
    # Footer avec informations
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ¤– SystÃ¨me", "RAG Actif")
    
    with col2:
        if hasattr(st.session_state, 'rag') and st.session_state.rag.chunks:
            st.metric("ğŸ“š Base", f"{len(st.session_state.rag.chunks)} chunks")
        else:
            st.metric("ğŸ“š Base", "Chargement...")
    
    with col3:
        st.metric("ğŸ“§ Email", "SMTP Relay")
    
    with col4:
        st.metric("ğŸš— Focus", "AUTOSAR + RFC")

if __name__ == "__main__":
    main()